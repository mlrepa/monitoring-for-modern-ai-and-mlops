{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QgumzOZ5wgec",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Monitoring with Evidently and MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "rByuPhg7wgei",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evidently.pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Text, Any, Dict\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ensemble, model_selection\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevidently\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpipeline\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolumn_mapping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ColumnMapping\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevidently\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Report\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevidently\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     16\u001b[39m     RegressionQualityMetric,\n\u001b[32m     17\u001b[39m     RegressionPredictedVsActualScatter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'evidently.pipeline'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "from pathlib import Path\n",
    "from typing import Text, Any, Dict\n",
    "from sklearn import ensemble, model_selection\n",
    "\n",
    "from evidently.pipeline.column_mapping import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import (\n",
    "    RegressionQualityMetric,\n",
    "    RegressionPredictedVsActualScatter,\n",
    "    RegressionPredictedVsActualPlot,\n",
    "    RegressionErrorPlot,\n",
    "    RegressionAbsPercentageErrorPlot,\n",
    "    RegressionErrorDistribution,\n",
    "    RegressionErrorNormality,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T07:14:19.165995Z",
     "iopub.status.busy": "2025-06-21T07:14:19.165478Z",
     "iopub.status.idle": "2025-06-21T07:14:20.186552Z",
     "shell.execute_reply": "2025-06-21T07:14:20.185962Z",
     "shell.execute_reply.started": "2025-06-21T07:14:19.165961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlflow_integrations_scout                0.1.0                 /Users/mikhailrozhkov/dev/nebius/nebius-ai-examples/crewai-examples/mlflow_integrations_scout\n",
      "mlflow-skinny                            2.21.2\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HiiUl3p8wgej",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data \n",
    "DATA_DIR = \"data\"\n",
    "FILENAME = \"raw_data.csv\"\n",
    "REPORTS_DIR = 'reports'\n",
    "\n",
    "# MLFlow\n",
    "MLFLOW_TRACKING_URI = \"http://localhost:5000\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zw5Tap_Xwgej",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VqGH1Mr6wgej",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "More information about the dataset can be found in UCI machine learning repository: https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset\n",
    "\n",
    "Acknowledgement: Fanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKX2YV19wgek",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download original dataset with: python src/load_data.py \n",
    "raw_data = pd.read_csv(f\"../{DATA_DIR}/{FILENAME}\")\n",
    "\n",
    "# Set datetime index \n",
    "raw_data = raw_data.set_index('dteday')\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dates for train data\n",
    "train_dates = ('2011-01-02 00:00:00','2011-03-06 23:00:00')\n",
    "\n",
    "# Define dates for inference batches\n",
    "prediction_batches = [ \n",
    "    ('2011-03-07 00:00:00','2011-03-13 23:00:00'),\n",
    "    ('2011-03-14 00:00:00','2011-03-20 23:00:00'),\n",
    "    ('2011-03-21 00:00:00','2011-03-27 23:00:00'), \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define column mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'cnt'\n",
    "prediction = 'prediction'\n",
    "datetime = 'dteday'\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed', 'mnth', 'hr', 'weekday']\n",
    "categorical_features = ['season', 'holiday', 'workingday', ]\n",
    "FEATURE_COLUMNS = numerical_features + categorical_features\n",
    "\n",
    "column_mapping = ColumnMapping()\n",
    "column_mapping.target = target\n",
    "column_mapping.prediction = prediction\n",
    "column_mapping.datetime = datetime\n",
    "column_mapping.numerical_features = numerical_features\n",
    "column_mapping.categorical_features = categorical_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = raw_data.loc['2011-01-01 00:00:00':'2011-01-28 23:00:00'].reset_index()\n",
    "\n",
    "print(sample_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    sample_data[numerical_features + categorical_features],\n",
    "    sample_data[target],\n",
    "    test_size=0.3\n",
    ")\n",
    "\n",
    "regressor = ensemble.RandomForestRegressor(random_state = 0, n_estimators = 50)\n",
    "regressor.fit(X_train, y_train) \n",
    "\n",
    "regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path('../models/model.joblib')\n",
    "joblib.dump(regressor, model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2rJyfTJ3wger",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Monitor Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the reference dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TZKZ_biHwgen",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the reference dataset\n",
    "reference_data = raw_data.loc[train_dates[0]:train_dates[1]]\n",
    "reference_data['prediction'] = regressor.predict(reference_data[FEATURE_COLUMNS])\n",
    "reference_data = reference_data.reset_index(drop=True)\n",
    "\n",
    "print(reference_data.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7eBBRAOTwger",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqljYICLwger",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "current_dates = prediction_batches[0]\n",
    "current_data = raw_data.loc[current_dates[0]:current_dates[1]]  \n",
    "\n",
    "print(current_data.shape)\n",
    "# current_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_prediction = regressor.predict(current_data[numerical_features + categorical_features])\n",
    "current_data['prediction'] = current_prediction\n",
    "current_data = current_data.reset_index(drop=True)\n",
    "\n",
    "print(current_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LvuPle_Nwges"
   },
   "outputs": [],
   "source": [
    "# Build the Model Monitoring report\n",
    "model_report = Report(metrics=[\n",
    "    RegressionQualityMetric(),\n",
    "    RegressionErrorPlot(),\n",
    "    RegressionErrorDistribution()\n",
    "])\n",
    "model_report.run(\n",
    "    reference_data=reference_data,\n",
    "    current_data=current_data,\n",
    "    column_mapping=column_mapping\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Monitoring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_monitoring_metrics(\n",
    "    regression_quality_report: Report\n",
    ") -> Dict:\n",
    "\n",
    "    metrics = {} \n",
    "    report_dict = regression_quality_report.as_dict()\n",
    "    \n",
    "    metrics['me'] = report_dict['metrics'][0]['result']['current']['mean_error']\n",
    "    metrics['mae'] = report_dict['metrics'][0]['result']['current'][\"mean_abs_error\"]\n",
    "    metrics['rmse'] = report_dict['metrics'][0]['result']['current'][\"rmse\"]\n",
    "    metrics['mape'] = report_dict['metrics'][0]['result']['current'][\"mean_abs_perc_error\"]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_metrics = get_model_monitoring_metrics(model_report)\n",
    "model_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmG0hvSwges",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dates = prediction_batches[1]\n",
    "current_data = raw_data.loc[current_dates[0]:current_dates[1]]  \n",
    "\n",
    "current_prediction = regressor.predict(current_data[numerical_features + categorical_features])\n",
    "current_data['prediction'] = current_prediction\n",
    "current_data = current_data.reset_index(drop=True)\n",
    "\n",
    "print(current_dates)\n",
    "print(current_data.shape)\n",
    "# current_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6tBIZDUwget",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Build the Model Monitoring report\n",
    "model_report = Report(metrics=[\n",
    "    RegressionQualityMetric(),\n",
    "    RegressionErrorPlot(),\n",
    "    RegressionErrorDistribution()\n",
    "])\n",
    "model_report.run(\n",
    "    reference_data=reference_data,\n",
    "    current_data=current_data,\n",
    "    column_mapping=column_mapping\n",
    ")\n",
    "\n",
    "model_metrics = get_model_monitoring_metrics(model_report)\n",
    "model_metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "q5lW24Xzwgex",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_IyYlM0wgey",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set up MLFlow Client\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "client = MlflowClient()\n",
    "print(f\"Client tracking uri: {client.tracking_uri}\")\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"Monitor Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTHU8eAqwgez",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run model monitoring for each batch of dates\n",
    "for current_dates in prediction_batches:\n",
    "    \n",
    "    print(f\"Current batch dates: {current_dates}\") \n",
    "\n",
    "    # Start a new Run for the batch\n",
    "    with mlflow.start_run(run_name=current_dates[1]) as run: \n",
    "        \n",
    "        # Show newly created run metadata info\n",
    "        print(\"Experiment id: {}\".format(run.info.experiment_id))\n",
    "        print(\"Run id: {}\".format(run.info.run_id))\n",
    "        print(\"Run name: {}\".format(run.info.run_name))\n",
    "            \n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"begin\", current_dates[0])\n",
    "        mlflow.log_param(\"end\", current_dates[1])\n",
    "        \n",
    "        # Make predictions for the current batch data\n",
    "        current_data = raw_data.loc[current_dates[0]:current_dates[1]]\n",
    "        current_prediction = regressor.predict(current_data[FEATURE_COLUMNS])\n",
    "        current_data['prediction'] = current_prediction\n",
    "        current_data = current_data.reset_index(drop=True)\n",
    "\n",
    "        # Build the Model Monitoring report\n",
    "        model_report = Report(metrics=[\n",
    "            RegressionQualityMetric(),\n",
    "            RegressionErrorPlot(),\n",
    "            RegressionErrorDistribution()\n",
    "        ])\n",
    "        model_report.run(\n",
    "            reference_data=reference_data,\n",
    "            current_data=current_data,\n",
    "            column_mapping=column_mapping\n",
    "        )\n",
    "        \n",
    "        # Log Metrics\n",
    "        model_metrics = get_model_monitoring_metrics(model_report)\n",
    "        mlflow.log_metrics(model_metrics)\n",
    "        \n",
    "        # Log Monitoring Report \n",
    "        monitoring_report_path = f\"../{REPORTS_DIR}/model_monitoring_report.html\"\n",
    "        model_report.save_html(monitoring_report_path)\n",
    "        mlflow.log_artifact(monitoring_report_path)\n",
    "        \n",
    "        print(run.info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
